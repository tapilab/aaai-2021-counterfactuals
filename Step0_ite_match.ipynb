{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match to get similar neighbors\n",
    "- Get top terms\n",
    "- Get context embedding\n",
    "- Find neighbors that:\n",
    "> have similar context (semantically) <br>\n",
    "> treatment word being substituted with other top terms <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import io, time\n",
    "from io import BytesIO\n",
    "from itertools import combinations, cycle, product\n",
    "from IPython.display import display\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tarfile\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "from scipy.sparse import hstack, lil_matrix\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1) # change None to -1\n",
    "\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import * # here import bert\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_structure import Dataset #, get_IMDB, get_kindle\n",
    "\n",
    "data_path = '/data/zwang/2020_S/Attention/Counterfactual/'\n",
    "\n",
    "# pickle.dump(ds_imdb, open(data_path+'imdb_embedding_1_01.pkl', 'wb'))\n",
    "# pickle.dump(df_imdb_ite, open(data_path+'imdb_ite_1_01.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kindle():\n",
    "    \"\"\"\n",
    "    Only use selected_train data\n",
    "    \"\"\"\n",
    "    df_kindle = pickle.load(open(\"/data/zwang/2020_S/Attention/Counterfactual/kindle_ct/causal_sents/kindle_data.pkl\",'rb'))\n",
    "    df_kindle = df_kindle[df_kindle['flag']=='selected_train']\n",
    "    df_kindle.reset_index(drop=True,inplace=True)\n",
    "    return df_kindle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The story was good, but I was getting very irritated at all the grammatical and spelling errors</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>selected_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REALLY enjoyed this series and am hopeful there will be a book 5 to give me some closure on some of the other characters</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>selected_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The lead character was successful and not as sad as usual, but the chemistry was lacking for me</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>selected_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I liked the characters though,  I would have loved to see how Dylan would have handled their relationship</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>selected_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Teenage, special needs child, ignorant ex-husband and a strong,weak,vulnerable, older woman</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>selected_train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                       text  rating  label            flag\n",
       "0  The story was good, but I was getting very irritated at all the grammatical and spelling errors                           2      -1      selected_train\n",
       "1  REALLY enjoyed this series and am hopeful there will be a book 5 to give me some closure on some of the other characters  4       1      selected_train\n",
       "2  The lead character was successful and not as sad as usual, but the chemistry was lacking for me                           1      -1      selected_train\n",
       "3  I liked the characters though,  I would have loved to see how Dylan would have handled their relationship                 2      -1      selected_train\n",
       "4  Teenage, special needs child, ignorant ex-husband and a strong,weak,vulnerable, older woman                               2      -1      selected_train"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((10000, 4), Counter({-1: 5000, 1: 5000}), None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kindle = get_kindle()\n",
    "df_kindle.shape, Counter(df_kindle.label), display(df_kindle.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Counterfactual:\n",
    "    def __init__(self, df_train, df_test, moniker):\n",
    "        display(df_train.head(1))\n",
    "        self.moniker = moniker\n",
    "        self.train = df_train\n",
    "        self.test = df_test\n",
    "        \n",
    "def get_IMDB():\n",
    "    \"\"\"\n",
    "    IMDB data split into sentences (len>1 and len<30)\n",
    "    \"\"\"\n",
    "#     df_imdb = pickle.load(open(data_path+'imdb/imdb_sentences/imdb_sents.pkl','rb'))\n",
    "    ds_imdb_ct = pickle.load(open(data_path+\"imdb_ct/sentiment/combined/paired/split_sents/ds_imdb.pkl\", \"rb\"))\n",
    "    df_imdb = ds_imdb_ct.train[['batch_id','text','label']]\n",
    "    df_imdb.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    return df_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Long, boring, blasphemous.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Never have I been so glad to see ending credits roll.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>Not good!</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>It is like claiming an Elvis actor is as good as the real King.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47</td>\n",
       "      <td>This movie is so bad, it can only be compared to the all-time worst \"comedy\": Police Academy 7.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_id                                                                                             text  label\n",
       "0  4         Long, boring, blasphemous.                                                                      -1    \n",
       "1  4         Never have I been so glad to see ending credits roll.                                           -1    \n",
       "2  40        Not good!                                                                                       -1    \n",
       "3  40        It is like claiming an Elvis actor is as good as the real King.                                 -1    \n",
       "4  47        This movie is so bad, it can only be compared to the all-time worst \"comedy\": Police Academy 7. -1    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((8173, 3), Counter({-1: 4059, 1: 4114}), None)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imdb = get_IMDB()\n",
    "df_imdb.shape, Counter(df_imdb.label), display(df_imdb.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_large_IMDB_sentences():\n",
    "    \"\"\"\n",
    "    IMDB sentences from the original large dataset\n",
    "    \"\"\"\n",
    "    df_imdb = pickle.load(open(data_path+'large_imdb_sents.pkl','rb'))\n",
    "    return df_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((160000, 4), Counter({1: 80000, -1: 80000}))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/data/zwang/2020_S/Attention/Counterfactual/imdb_ct/sentiment/orig/eighty_percent/sentences/'\n",
    "df_imdb = get_large_IMDB_sentences()\n",
    "df_imdb.shape, Counter(df_imdb.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The characters are interesting, vibrant with primary colours and all.</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;the best way to watch this film is to not expect what you have seen in the past by Miyazaki.</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This one's worth watching more than once, and showing to all your friends.</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>That's not true in Japanese horror.</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No one could have played this role any better that Jack Webb.</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                       text  label   type  length\n",
       "0  The characters are interesting, vibrant with primary colours and all.                                     1      train  10    \n",
       "1  <br /><br />the best way to watch this film is to not expect what you have seen in the past by Miyazaki.  1      test   22    \n",
       "2  This one's worth watching more than once, and showing to all your friends.                                1      test   14    \n",
       "3  That's not true in Japanese horror.                                                                       1      test   7     \n",
       "4  No one could have played this role any better that Jack Webb.                                             1      test   12    "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_vectorize(df):\n",
    "    \"\"\"\n",
    "    Vectorize text\n",
    "    min_df = 10: agree with min_df for ite features\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer(min_df=5, binary=True, max_df=.8)\n",
    "    X = vec.fit_transform(df.text)\n",
    "    print(X.shape)\n",
    "    y = df.label.values\n",
    "    feats = np.array(vec.get_feature_names())\n",
    "    \n",
    "    return X, y, vec, feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_terms(dataset, coef_thresh, placebo_thresh, C=1):\n",
    "    \"\"\"\n",
    "    Fit classifier, print top-200 terms;\n",
    "    Top features: abs(coef) >= thresh\n",
    "    Placebos: abs(coef) <= thresh\n",
    "    \"\"\"\n",
    "    clf = LogisticRegression(class_weight='auto', C=C, solver='lbfgs', max_iter=1000)\n",
    "    clf.fit(dataset.X, dataset.y)\n",
    "    \n",
    "#     print_coef(clf, dataset.feats, n=100)\n",
    "    #print('dummy coef= %.3f' % clf.coef_[0][dataset.vec.vocabulary_[DUMMY_TERM]])\n",
    "    \n",
    "    top_feature_idx = np.where(abs(clf.coef_[0]) >= coef_thresh)[0]\n",
    "    placebo_feature_idx = np.where(abs(clf.coef_[0]) <= placebo_thresh)[0]\n",
    "    \n",
    "    return top_feature_idx, placebo_feature_idx, np.array([float(\"%.3f\" % c) for c in clf.coef_[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wd_context(sentence, word, window=0):\n",
    "    \"\"\"\n",
    "    Return left context and right context\n",
    "    If window > 0: return n words to the left / right;\n",
    "    If window == 0: return all words to the left / right.\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    toks = sentence.split()\n",
    "    \n",
    "    for i, t in enumerate(toks):\n",
    "        if re.search(r'(?i)\\b%s\\b' % word, t): # if find the word, then take left-n and right-n words\n",
    "            if window > 0:\n",
    "                context = ' '.join(toks[max(0, i-window):min(i+window+1, len(toks))])\n",
    "                left_context = ' '.join(toks[max(0, i-window):i])\n",
    "                right_context = ' '.join(toks[(i+1):min(i+window+1, len(toks))])\n",
    "            elif window == 0:\n",
    "                context = ' '.join(toks[:i] + toks[i+1:])\n",
    "                left_context = ' '.join(toks[:i])\n",
    "                right_context = ' '.join(toks[(i+1):])\n",
    "\n",
    "#             return re.sub(r'(?i)\\b%s\\b' % word, ' ', context, re.IGNORECASE)\n",
    "            return left_context, right_context, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nice movie under the direction',\n",
       " 'Spielberg',\n",
       " 'nice movie under the direction Spielberg')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_wd_context(sentence='nice movie under the direction of Spielberg', word='of', window=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEdit:\n",
    "    def __init__(self, remove_wd, sentence_idx, left_context, right_context, context, label):\n",
    "        \n",
    "        self.sentence_idx = sentence_idx\n",
    "        self.remove_wd = remove_wd\n",
    "        self.context = context\n",
    "        self.label = label\n",
    "        self.left_context = left_context\n",
    "        self.right_context = right_context\n",
    "        \n",
    "      \n",
    "    def __repr__(self):\n",
    "        \" returns a printable string representation of an object\"\n",
    "        if(len(str(self.left_context).strip() + str(self.right_context).strip())==0):\n",
    "            return '%s ||| %s \\n' % (str(self.context), str(self.label))\n",
    "        else:\n",
    "            return '%s ||| %s ||| %s ||| %s \\n' % (str(self.remove_wd), str(self.left_context), str(self.right_context), str(self.label))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wd_unk_sentences(X, sentences, labels, vec, remove_wd_list, exclude_sent_idx, window=0):\n",
    "    \"\"\"\n",
    "    unk sentence: sentence after removing the word\n",
    "    remove_wd_list:\n",
    "        - top_words: abs(coef) > thresh\n",
    "        - placebo_words: abs(coef) < thresh\n",
    "    exclude_sent_idx:\n",
    "        - sentences to exclude\n",
    "    \n",
    "    for each word to be removed\n",
    "      for each treatment sentence (sentence containing this word)\n",
    "        make a copy of the sentence with this word removed\n",
    "        \n",
    "    returns:\n",
    "       unk_sentences: list of SentenceEdit objects\n",
    "       word2sentences: dict from word to list of SentenceEdit objects\n",
    "    \"\"\"\n",
    "    word2sentences = defaultdict(list)\n",
    "    unk_sentences = []\n",
    "    containing_sents = []\n",
    "    \n",
    "    for word in remove_wd_list: # iterate over all treatment word\n",
    "        wi = vec.vocabulary_[word]\n",
    "        for si in X[:,wi].nonzero()[0]: # iterate over sentences containing current treatment word\n",
    "            if(si not in exclude_sent_idx): # not contain any top words\n",
    "                left_context, right_context, context = get_wd_context(sentences[si], word, window=0) # context within window=5\n",
    "                sent_obj = SentenceEdit(word, si, left_context, right_context, context, labels[si])\n",
    "                \n",
    "                word2sentences[word].append(sent_obj)\n",
    "                unk_sentences.append(sent_obj)\n",
    "                containing_sents.append(si)\n",
    "                \n",
    "#     print('%d sentences with placebo terms\\n' % len(unk_sentences))\n",
    "    return unk_sentences, word2sentences, list(set(containing_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(df):\n",
    "    \"\"\"\n",
    "    Construct SentenceEdit object for all sentences\n",
    "    \"\"\"\n",
    "    df['i_th'] = range(df.shape[0])\n",
    "    all_sentences = []\n",
    "    for ri, row in df.iterrows():\n",
    "        sent_obj = SentenceEdit('', row['i_th'], '' , '',  row['text'], row['label'])\n",
    "        all_sentences.append(sent_obj)\n",
    "        \n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bert():\n",
    "    return (BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "            BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True, output_attentions=True))\n",
    "\n",
    "# bert representation of each sentence.\n",
    "def embed_sentence(sentence, sentence_model, tokenizer):\n",
    "    \"\"\"\n",
    "    # bert_tokenizer.vocab_size\n",
    "    # bert_tokenizer.tokenize(sentence)\n",
    "    # bert_tokenizer.convert_tokens_to_ids('on')\n",
    "    # bert_tokenizer.convert_ids_to_tokens(102)\n",
    "    # each sentence is encoded as a 3072 vec: 768 * 4 (concat last four layers)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # sentence_model returns (logit output layer, pooler_output, hidden states, attentions)\n",
    "        hidden_states = sentence_model(torch.tensor([tokenizer.encode(sentence, add_special_tokens=True)]))[2]\n",
    "        #last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
    "        last_four_layers = [hidden_states[i] for i in (0,1,2,3)] # list of 4 element, each element is [1,16,768]\n",
    "        # cast layers to a tuple and concatenate over the last dimension\n",
    "        cat_hidden_states = torch.cat(tuple(last_four_layers), dim=-1) # [1,16,3072]\n",
    "        cat_sentence_embedding = torch.mean(cat_hidden_states, dim=1).squeeze() # 3027\n",
    "        #return(torch.mean(hidden_states[-1], dim=1).squeeze()) # average word embeddings in last layer\n",
    "        return cat_sentence_embedding.numpy()                         # average last 4 layers\n",
    "        \n",
    "def embed_all_sentences(sentences, bert_tokenizer=None, sentence_model=None):\n",
    "    \"\"\"\n",
    "    Each sentence is an object of SentenceEdit;\n",
    "    Adding embedding attribute for the object;\n",
    "    \"\"\"\n",
    "    if not bert_tokenizer:        \n",
    "        bert_tokenizer, sentence_model = load_bert()\n",
    "    for s in tqdm(sentences):\n",
    "#         s.left_embedding = embed_sentence(s.left_context, sentence_model, bert_tokenizer)\n",
    "#         s.right_embedding = embed_sentence(s.right_context, sentence_model, bert_tokenizer)\n",
    "        s.context_embedding = embed_sentence(s.context, sentence_model, bert_tokenizer)\n",
    "        s.word_embedding = embed_sentence(s.remove_wd, sentence_model, bert_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_embedding():\n",
    "    \"\"\"\n",
    "    window_size = 0: left_context + right_context\n",
    "    window_size = n: left / right n words as context\n",
    "    \"\"\"\n",
    "    random.seed(42)\n",
    "    datasets = []\n",
    "    for get_data_df, moniker, coef_thresh, placebo_thresh in [\n",
    "            (get_IMDB, 'imdb', 1.0, 0.1),\n",
    "#             (get_kindle, 'kindle', 1.0, 0.1),\n",
    "#             (get_toxic_comment, 'toxic', 1.0, 0.05), # limit_len == False\n",
    "#             (get_toxic_tw, 'toxic_tw', 0.7, 0.2),\n",
    "#             (get_TV_data, 'tv', 0.9, 0.1),\n",
    "        ]: \n",
    "        \n",
    "        df = get_data_df()\n",
    "        X, y, vec, feats = simple_vectorize(df) # vectorize text\n",
    "        ds = Dataset(X, y, vec, df, moniker) # construct dataset object\n",
    "        \n",
    "        print('%s dataset, %d instances' % (moniker,len(df)))\n",
    "        print('Label distribution: %s' % str(Counter(y).items()))\n",
    "        print('Feature matrix: %s' % str(X.shape))\n",
    "        \n",
    "        # get top / placebo features\n",
    "        ds.top_feature_idx, ds.placebo_feature_idx, ds.coef = get_top_terms(ds, coef_thresh=coef_thresh, \n",
    "                                                                            placebo_thresh=placebo_thresh, C=1)\n",
    "        ds.top_features = feats[ds.top_feature_idx]\n",
    "        ds.placebo_features = feats[ds.placebo_feature_idx]\n",
    "        print('\\n%d top terms: %d pos, %d neg\\n' % (len(ds.top_features), \n",
    "                                                    len(np.where(ds.coef[ds.top_feature_idx]>0)[0]), \n",
    "                                                    len(np.where(ds.coef[ds.top_feature_idx]<0)[0])))\n",
    "        \n",
    "#         print('\\n%d placebo terms: %d pos, %d neg\\n' % (len(ds.placebo_features), \n",
    "#                                                         len(np.where(ds.coef[ds.placebo_feature_idx]>0)[0]), \n",
    "#                                                         len(np.where(ds.coef[ds.placebo_feature_idx]<0)[0])))\n",
    "        \n",
    "        # embed texts\n",
    "        print('getting treat sentences')                                                                   \n",
    "        ds.topwd_unk_sentences_list, ds.topwd_unk_sentences_dict, topwd_idx = get_wd_unk_sentences(X,\n",
    "                                                                                                   df.text,df.label,\n",
    "                                                                                                   vec,ds.top_features,\n",
    "                                                                                                   exclude_sent_idx=[],\n",
    "                                                                                                   window=0)\n",
    "        print('%d unk sentences with top terms\\n' % len(ds.topwd_unk_sentences_list))\n",
    "        embed_all_sentences(ds.topwd_unk_sentences_list) \n",
    "#         embed_all_sentences_USE(ds.topwd_unk_sentences_list)\n",
    "\n",
    "#         print('getting placebo sentences')     \n",
    "#         ds.placebowd_unk_sentences_list, ds.placebowd_unk_sentences_dict, placebo_wd_idx = get_wd_unk_sentences(X,\n",
    "#                                                                                                                 df.text,\n",
    "#                                                                                                                 df.label,\n",
    "#                                                                                                                 vec,\n",
    "#                                                                                                                 ds.placebo_features,\n",
    "#                                                                                                                 topwd_idx,\n",
    "#                                                                                                                 window=window_size)\n",
    "#         print('%d unk sentences with placebo terms\\n' % len(ds.placebowd_unk_sentences_list))\n",
    "#         embed_all_sentences(ds.placebowd_unk_sentences_list) \n",
    "        \n",
    "#         print('getting vocab sentences')     \n",
    "#         ds.vocab_unk_sentences_list, ds.vocab_unk_sentences_dict, vocab_wd_idx = get_wd_unk_sentences(X,df.text,df.label,vec,ds.vec.get_feature_names(),exclude_sent_idx=[])\n",
    "#         print('%d unk sentences with vocab terms\\n' % len(ds.vocab_unk_sentences_list))\n",
    "#         embed_all_sentences(ds.vocab_unk_sentences_list) \n",
    "        \n",
    "        print('getting all sentences as control')\n",
    "        ds.all_sentences = get_all_sentences(df)\n",
    "        print('%d control sentences\\n\\n' % len(ds.all_sentences))\n",
    "        embed_all_sentences(ds.all_sentences)    \n",
    "        \n",
    "#         pickle.dump(ds, open(data_path+'large_imdb_sents_embedding_1_01.pkl', 'wb'))\n",
    "#         pickle.dump(ds, open(data_path+'kindle/kindle_sentences/kindle_sents_embedding_1_01.pkl', 'wb'))\n",
    "        \n",
    "        return ds\n",
    "        \n",
    "        \n",
    "#         datasets.append(ds)\n",
    "        \n",
    "#     return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adorable ||| The babies are ||| and it's fun watching them play and grow. ||| 1 "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_imdb.topwd_unk_sentences_list[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8173, 2865)\n",
      "new dataset with 8173 records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Long, boring, blasphemous.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_id                        text  label\n",
       "0  4         Long, boring, blasphemous. -1    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb dataset, 8173 instances\n",
      "Label distribution: dict_items([(-1, 4059), (1, 4114)])\n",
      "Feature matrix: (8173, 2865)\n",
      "\n",
      "197 top terms: 99 pos, 98 neg\n",
      "\n",
      "getting treat sentences\n",
      "6392 unk sentences with top terms\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c53d769d8b0402cb84a3185c0e3b991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6392.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "getting all sentences as control\n",
      "8173 control sentences\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e935a158d87f45c7965968ec86b02ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8173.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12.552439943949382\n"
     ]
    }
   ],
   "source": [
    "start = time.time() # imdb: 3 hours; toxic: 6 hours\n",
    "ds_imdb = get_data_embedding() # kindle, 20 min\n",
    "# pickle.dump(ds_kindle, open(data_path+'kindle_ct/ITE/ds_kindle_emb.pkl','wb'))\n",
    "# pickle.dump(ds_imdb, open(data_path+'imdb_ct/sentiment/combined/paired/ITE/ds_imdb_emb.pkl','wb'))\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ds_kindle = pickle.load(open(data_path+'kindle/kindle_sentences/kindle_sents_embedding_1_01.pkl', 'rb'))\n",
    "# term_stats = {}\n",
    "# term_stats['term'] = ds_kindle.feats[ds_kindle.top_feature_idx]\n",
    "# term_stats['coef'] = ds_kindle.coef[ds_kindle.top_feature_idx]\n",
    "# term_df = pd.DataFrame(term_stats)\n",
    "# term_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term_df.to_csv(data_path+'kindle/kindle_sentences/annotate_top_terms.csv' , index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ITE match\n",
    "- context_A + treat_wd_A\n",
    "- context_B + placebo_wd_B\n",
    "- similarity(context_A, context_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_data = pickle.load(open(data_path+'imdb_embedding_1_01.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_embedding(obj,flag):\n",
    "    \"\"\"\n",
    "    sentence embedding from left and right context embeddings\n",
    "    flag = ['left','right','word']\n",
    "    emb_by: avg, concat\n",
    "    \"\"\"\n",
    "    \n",
    "    emb = []\n",
    "    for f in flag:\n",
    "        if(f == 'left' and len(obj.left_context.strip())>0):\n",
    "            emb.append(obj.left_embedding)\n",
    "        elif(f == 'right' and len(obj.right_context.strip())>0):\n",
    "            emb.append(obj.right_embedding)\n",
    "        elif(f == 'word' and len(obj.remove_wd.strip())>0):\n",
    "            emb.append(obj.word_embedding)\n",
    "\n",
    "    if(len(emb)>0):\n",
    "        return np.mean(emb, axis=0)\n",
    "    else:\n",
    "        return np.zeros(len(obj.left_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_embedding(c_sent_objlist[3478],flag=['left','right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_embedding(obj,flag):\n",
    "    \"\"\"\n",
    "    sentence embedding from left and right context embeddings\n",
    "    flag = ['left','right','word']\n",
    "    \"\"\"\n",
    "    \n",
    "    emb = []\n",
    "    for f in flag:\n",
    "        if(f == 'left'):\n",
    "            emb.append(obj.left_embedding)\n",
    "        elif(f == 'right'):\n",
    "            emb.append(obj.right_embedding)\n",
    "        elif(f == 'word' and len(obj.remove_wd.strip())>0):\n",
    "            emb.append(obj.word_embedding)\n",
    "\n",
    "    if(len(emb)>0):\n",
    "        return np.hstack(emb)\n",
    "    else:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_match(t_sent_obj, t_sent_objlist, c_sent_objlist, similarity='cosine', emb_by='context',min_sim=.7):\n",
    "    \"\"\"\n",
    "    get the most similar match: (context, treat_wd) VS (context, placebo_wd)\n",
    "    \n",
    "    t_sent_obj: context_A + treat_wd\n",
    "    c_sent_objlist: a list of context_B + placebo_wd\n",
    "    similarity(context_A, context_B)\n",
    "    \n",
    "    emb_by: avg, concat, context\n",
    "    for context_A:\n",
    "        sort similarity score of (context_A, context of all placebo words in the dataset) in descending order\n",
    "        if((sentence_A != sentence_B) and (cos(context_A,context_B)>0.7) and (word_A != word_B)):\n",
    "            context_B is a match for context_A\n",
    "    \n",
    "    diff: difference between sentence_A.label - sentence_B.label\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # similarity between current treatment context with all other contexts\n",
    "    \n",
    "    random.seed(42)\n",
    "    if(len(c_sent_objlist) > 50000):\n",
    "        c_sent_obj_smp = random.sample(c_sent_objlist,50000)\n",
    "    else:\n",
    "        c_sent_obj_smp = c_sent_objlist\n",
    "    \n",
    "    \n",
    "    if(emb_by == 'context'):\n",
    "        u = [t_sent_obj.context_embedding]\n",
    "        v = [c_sent_obj.context_embedding for c_sent_obj in c_sent_obj_smp]\n",
    "        control_sims = cosine_similarity(u, v)[0]\n",
    "        treat_sims = cosine_similarity(u,[c_sent_smp.context_embedding for c_sent_smp in t_sent_objlist])[0]\n",
    "    elif(emb_by == 'avg'):\n",
    "        u = [avg_embedding(t_sent_obj,['left', 'right'])]\n",
    "        v = [avg_embedding(c_sent_obj,['left', 'right']) for c_sent_obj in c_sent_obj_smp]\n",
    "    elif(emb_by == 'concat'):\n",
    "        u = [concat_embedding(t_sent_obj,['left', 'right'])]\n",
    "        v = [concat_embedding(c_sent_obj,['left', 'right']) for c_sent_obj in c_sent_obj_smp]\n",
    "    \n",
    "    \n",
    "    treat_match = [] # find 20 most similar matches\n",
    "    n_treat = 0\n",
    "    for t_sent_smp, sim in sorted(zip(t_sent_objlist, treat_sims), key=lambda x: -x[1]):\n",
    "        if((t_sent_smp.sentence_idx != t_sent_obj.sentence_idx) and (t_sent_smp.remove_wd != t_sent_obj.remove_wd)):\n",
    "            treat_match.append((t_sent_smp, float(\"%.3f\" % sim)))\n",
    "            n_treat += 1\n",
    "            if(n_treat==10):\n",
    "                break\n",
    "           \n",
    "    control_match = []\n",
    "    n_control = 0\n",
    "    for c_sent_smp, sim in sorted(zip(c_sent_obj_smp, control_sims), key=lambda x: -x[1]): \n",
    "        if((c_sent_smp.sentence_idx != t_sent_obj.sentence_idx) and (t_sent_obj.remove_wd not in c_sent_smp.context)):\n",
    "#             if (not re.search(r'(?i)\\b%s\\b' % t_sent_obj.remove_wd, c_sent_obj.left_context+' '+c_sent_obj.right_context)): # the treat word is not in control context\n",
    "#                 if (not re.search(r'(?i)\\b%s\\b' % c_sent_obj.remove_wd, t_sent_obj.left_context+' '+t_sent_obj.right_context)):\n",
    "            control_match.append((c_sent_smp, float(\"%.3f\" % sim)))\n",
    "            n_control += 1\n",
    "            if(n_control==10):\n",
    "                break\n",
    "            \n",
    "    return treat_match, control_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9999998 , 0.7434988 , 0.73416686, ..., 0.76551396, 0.65273   ,\n",
       "       0.775444  ], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_data = ds_imdb\n",
    "t_sent_objlist = ds_data.topwd_unk_sentences_list\n",
    "c_sent_objlist = ds_data.topwd_unk_sentences_list\n",
    "\n",
    "sims = cosine_similarity([t_sent_objlist[0].context_embedding], [c_sent_obj.context_embedding for c_sent_obj in c_sent_objlist])[0]\n",
    "sims\n",
    "# diff, match_list, match_sim, match_label = get_sentence_match(t_sent_objlist[2000], c_sent_objlist, min_sim=.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- most similar match with same / opposite label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_matches(ds_data, matchby='treat', emb_by='context', min_sim=0.01):\n",
    "    \"\"\"\n",
    "    get one most similar match for each sentence\n",
    "    t_sent_objlist: a list of SentenceEdit object, sentences with top words\n",
    "    c_sent_objlist: a list of SentenceEdit object, sentences with top / placebo / '' words\n",
    "    matchby = treat / control / placebo\n",
    "    \"\"\"\n",
    "    t_sent_objlist = ds_data.topwd_unk_sentences_list\n",
    "    \n",
    "    if(matchby == 'treat'):\n",
    "        c_sent_objlist = ds_data.topwd_unk_sentences_list\n",
    "    elif(matchby == 'placebo'):\n",
    "        c_sent_objlist = ds_data.placebowd_unk_sentences_list\n",
    "    elif(matchby == 'vocab'):\n",
    "        c_sent_objlist = ds_data.vocab_unk_sentences_list\n",
    "    elif(matchby == 'control'):\n",
    "        c_sent_objlist = ds_data.all_sentences\n",
    "    else:\n",
    "        print(\"combinations of treat + placebo + control\")\n",
    "    \n",
    "    matched_ites = []   \n",
    "    for t_sent_obj in tqdm(t_sent_objlist):  # t_sent_objlist, c_sent_objlist\n",
    "        treat_match, control_match = get_sentence_match(t_sent_obj, ds_data.topwd_unk_sentences_list, ds_data.all_sentences, \n",
    "                                                   similarity='cosine', emb_by=emb_by, min_sim=min_sim)\n",
    "        \n",
    "#         treat_match, control_match = get_sentence_match(t_sent_obj, ds_data.topwd_unk_sentences_dict[t_sent_obj.remove_wd], c_sent_objlist, \n",
    "#                                                    similarity='cosine', emb_by=emb_by, min_sim=min_sim)\n",
    "            \n",
    "        matched_ites.append(\n",
    "            {\n",
    "                'term': t_sent_obj.remove_wd,\n",
    "#                 'sentence_id': t_sent_obj.sentence_idx,\n",
    "                'sentence': t_sent_obj,\n",
    "                'treat_match': treat_match,\n",
    "                'control_match': control_match,\n",
    "            }\n",
    "            \n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(matched_ites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8c9163560b4ef9bfc5acb4a3dee189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13014.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "76.70414376656214\n"
     ]
    }
   ],
   "source": [
    "start = time.time() # 10pm ~7am\n",
    "# ds_imdb_window5 = pickle.load(open(data_path+'imdb/imdb_embedding_1_01_window5.pkl', 'rb'))\n",
    "# print('vocab')\n",
    "# df_imdb_vocab = get_data_matches(ds_imdb, matchby='vocab')\n",
    "# print('control')\n",
    "# df_imdb_control = get_data_matches(ds_imdb, matchby='control')\n",
    "\n",
    "# match with context\n",
    "# ds_kindle = pickle.load(open(data_path+'kindle/kindle_embedding_1_01.pkl', 'rb'))\n",
    "ds_data = ds_kindle # 8 hours\n",
    "df_ite_match = get_data_matches(ds_data, matchby='treat', emb_by='context') # 90 min for kindle, 22 min for imdb\n",
    "# pickle.dump(df_ite_match, open(data_path+'kindle_ct/ITE/kindle_ite_match.pkl','wb'))\n",
    "# pickle.dump(df_ite_match, open(data_path+'imdb_ct/sentiment/combined/paired/ITE/imdb_ite_match.pkl','wb'))\n",
    "end = time.time() \n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>sentence</th>\n",
       "      <th>treat_match</th>\n",
       "      <th>control_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13013</th>\n",
       "      <td>wrong</td>\n",
       "      <td>wrong ||| The poor victims were just in the ||| place at the wrong time ||| -1 \\n</td>\n",
       "      <td>[(series ||| The other books in the ||| are just as good ||| 1 \\n, 0.832), (great ||| The others were ||| but I had a hard time reading the story with her in it ||| -1 \\n, 0.823), (boring ||| The book was ||| and the characters were just as bad ||| -1 \\n, 0.815), (boring ||| I found it ||| and the story was all over the place ||| -1 \\n, 0.811), (falls ||| I WAS HAPPY WITH THE PRICE BUT IT ||| RIGHT OUT OF THE CASE ||| -1 \\n, 0.81), (editing ||| The ||| was so poor, however, that it was hard to get through the story ||| -1 \\n, 0.809), (love ||| The whole ||| story was thrown in at the last minute ||| -1 \\n, 0.807), (series ||| Once more a good ||| written about England and the problems of the average person during that time ||| 1 \\n, 0.806), (love ||| The way the unethical partner was handled was well deserved and in the end, the ||| of the family members won out ||| 1 \\n, 0.805), (love ||| When you thought that it was impossible to ||| 2 men at the same time ||| 1 \\n, 0.803)]</td>\n",
       "      <td>[(The other books in the series are just as good ||| 1 \\n, 0.815), (The book was boring, and the characters were just as bad ||| -1 \\n, 0.81), (The others were great but I had a hard time reading the story with her in it ||| -1 \\n, 0.81), (I WAS HAPPY WITH THE PRICE BUT IT FALLS RIGHT OUT OF THE CASE ||| -1 \\n, 0.81), (I found it boring and the story was all over the place ||| -1 \\n, 0.805), (The whole love story was thrown in at the last minute ||| -1 \\n, 0.803), (It was not funny at all and the romance was the worst ||| -1 \\n, 0.803), (The way the unethical partner was handled was well deserved and in the end, the love of the family members won out ||| 1 \\n, 0.801), (Once more a good series written about England and the problems of the average person during that time ||| 1 \\n, 0.8), (It was nice to see the good guys win in the end ||| 1 \\n, 0.798)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        term                                                                           sentence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     treat_match                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   control_match\n",
       "13013  wrong  wrong ||| The poor victims were just in the ||| place at the wrong time ||| -1 \\n  [(series ||| The other books in the ||| are just as good ||| 1 \\n, 0.832), (great ||| The others were ||| but I had a hard time reading the story with her in it ||| -1 \\n, 0.823), (boring ||| The book was ||| and the characters were just as bad ||| -1 \\n, 0.815), (boring ||| I found it ||| and the story was all over the place ||| -1 \\n, 0.811), (falls ||| I WAS HAPPY WITH THE PRICE BUT IT ||| RIGHT OUT OF THE CASE ||| -1 \\n, 0.81), (editing ||| The ||| was so poor, however, that it was hard to get through the story ||| -1 \\n, 0.809), (love ||| The whole ||| story was thrown in at the last minute ||| -1 \\n, 0.807), (series ||| Once more a good ||| written about England and the problems of the average person during that time ||| 1 \\n, 0.806), (love ||| The way the unethical partner was handled was well deserved and in the end, the ||| of the family members won out ||| 1 \\n, 0.805), (love ||| When you thought that it was impossible to ||| 2 men at the same time ||| 1 \\n, 0.803)]  [(The other books in the series are just as good ||| 1 \\n, 0.815), (The book was boring, and the characters were just as bad ||| -1 \\n, 0.81), (The others were great but I had a hard time reading the story with her in it ||| -1 \\n, 0.81), (I WAS HAPPY WITH THE PRICE BUT IT FALLS RIGHT OUT OF THE CASE ||| -1 \\n, 0.81), (I found it boring and the story was all over the place ||| -1 \\n, 0.805), (The whole love story was thrown in at the last minute ||| -1 \\n, 0.803), (It was not funny at all and the romance was the worst ||| -1 \\n, 0.803), (The way the unethical partner was handled was well deserved and in the end, the love of the family members won out ||| 1 \\n, 0.801), (Once more a good series written about England and the problems of the average person during that time ||| 1 \\n, 0.8), (It was nice to see the good guys win in the end ||| 1 \\n, 0.798)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ite_match.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ite = {}\n",
    "df_ite['treat'] = df_ite_treat\n",
    "df_ite['placebo'] = df_ite_placebo\n",
    "pickle.dump(df_ite,open(data_path+'kindle/kindle_ite_byconcat.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'15' in ds_kindle.top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>treat_obj</th>\n",
       "      <th>control_ids</th>\n",
       "      <th>control_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>308</td>\n",
       "      <td>15 ||| read if he had chosen ||| legends and elaborated on them ||| -1 \\n</td>\n",
       "      <td>[188, 4540, 4370, 3299, 2749, 1507, 6175, 857, 2522, 5803, 6059, 1410, 6108, 1620, 4390, 234, 2982, 5982, 2429, 5148, 7162, 5755, 3136, 175, 6172, 4643, 5773, 80, 2985, 3574, 5553, 3364, 1503, 6244, 2074, 1541, 6586, 2010, 2751, 164, 6224, 2025, 6096, 1257, 4366, 5356, 4996, 7121, 1277, 3651, 3076, 3783, 6974, 692, 733, 744, 629, 1044, 5874, 2333, 859, 1677, 2023, 2729, 5675, 137, 6329, 846, 2071, 4474, 2831, 2805, 880, 1125, 6408, 1922, 6048, 6405, 4636, 5663, 2176, 3118, 2882, 1791, 1183, 745, 2444, 7165, 6163, 1130, 6989, 4187, 105, 616, 3619, 4642, 3760, 6174, 5863, 1865]</td>\n",
       "      <td>[0.774431, 0.7657779, 0.7654418, 0.7586538, 0.75765145, 0.75622255, 0.75387245, 0.7537848, 0.752055, 0.7515459, 0.751132, 0.75104064, 0.75036454, 0.75020015, 0.7485354, 0.74835086, 0.74833584, 0.74815947, 0.7479827, 0.747982, 0.7470428, 0.7463633, 0.74589694, 0.745704, 0.745267, 0.7447803, 0.744725, 0.74398893, 0.7432944, 0.7432766, 0.7427939, 0.74252474, 0.742417, 0.74239707, 0.7420739, 0.7419039, 0.7415648, 0.74129796, 0.74038553, 0.73997355, 0.7397634, 0.73882425, 0.7388134, 0.73840004, 0.73831725, 0.738283, 0.737738, 0.7374506, 0.7374104, 0.7374099, 0.7373049, 0.73705024, 0.73661757, 0.73660856, 0.73660856, 0.73660856, 0.73644245, 0.736059, 0.73586774, 0.73576295, 0.7357268, 0.7356725, 0.7353916, 0.73535186, 0.7352672, 0.73520756, 0.73473483, 0.73451936, 0.73436654, 0.73417914, 0.73410475, 0.73389065, 0.73387575, 0.73384523, 0.7338085, 0.7337475, 0.7335499, 0.73337823, 0.733353, 0.73333454, 0.7333176, 0.7330762, 0.73299396, 0.73235583, 0.73223346, 0.73214984, 0.73211503, 0.73199534, 0.7319912, 0.73147464, 0.7314663, 0.7311471, 0.7310847, 0.73101175, 0.7308252, 0.7308156, 0.7305696, 0.73056245, 0.73031694, 0.7302091]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>434</td>\n",
       "      <td>15 ||| free but i want my ||| min of life back ||| -1 \\n</td>\n",
       "      <td>[2127, 2170, 3125, 2113, 718, 2121, 6895, 6026, 2160, 6782, 243, 6664, 2126, 1111, 5224, 6805, 629, 6357, 2184, 1812, 2084, 5175, 7037, 1179, 2259, 3416, 3799, 1251, 5773, 6337, 3363, 1342, 5147, 6274, 6172, 4676, 6713, 1554, 2158, 6884, 473, 614, 708, 3676, 3362, 4160, 2167, 2924, 1427, 3539, 7021, 4255, 6251, 1193, 5111, 2177, 5244, 3181, 7123, 3454, 3931, 5234, 7141, 2020, 3555, 1277, 2216, 3248, 3221, 2699, 1762, 6598, 6519, 110, 6553, 7137, 253, 5470, 5478, 1354, 3122, 175, 3212, 3156, 6761, 4560, 6741, 4354, 3952, 3708, 898, 4574, 1191, 1207, 3990, 2236, 244, 773, 3562, 6423]</td>\n",
       "      <td>[0.80159783, 0.80159783, 0.75432986, 0.7523407, 0.7521267, 0.7503412, 0.7487483, 0.74851257, 0.7460893, 0.7457087, 0.74384177, 0.7437999, 0.7436033, 0.7432044, 0.74240184, 0.74160695, 0.74076843, 0.74056995, 0.7400458, 0.73904514, 0.7377364, 0.7349162, 0.7347597, 0.7345468, 0.73429215, 0.73221064, 0.7314495, 0.72963405, 0.7294752, 0.7294397, 0.72929406, 0.7286548, 0.72816515, 0.72712684, 0.72669935, 0.7263777, 0.7255297, 0.72502124, 0.7247044, 0.72465146, 0.72463375, 0.72456336, 0.7242066, 0.7236022, 0.72344446, 0.72297347, 0.7227949, 0.7224593, 0.72219646, 0.722188, 0.7218343, 0.7215353, 0.721472, 0.7212204, 0.7209994, 0.72019124, 0.71988475, 0.719862, 0.71956116, 0.7195289, 0.71947646, 0.7192878, 0.7187964, 0.7187561, 0.7184754, 0.7182376, 0.7180848, 0.7177154, 0.7175374, 0.7174197, 0.7173748, 0.7172474, 0.7171042, 0.7170011, 0.71683997, 0.7166271, 0.7165545, 0.7164749, 0.7164495, 0.71596324, 0.71592975, 0.7158358, 0.7158158, 0.7157743, 0.7155573, 0.71547824, 0.7154068, 0.7153419, 0.7153045, 0.7151293, 0.7151145, 0.71509135, 0.71494764, 0.714921, 0.7148969, 0.71487635, 0.71478707, 0.71476185, 0.71471786, 0.7146066]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  term  sentence_id                                                                  treat_obj                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   control_ids                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        control_cos\n",
       "0  15   308          15 ||| read if he had chosen ||| legends and elaborated on them ||| -1 \\n  [188, 4540, 4370, 3299, 2749, 1507, 6175, 857, 2522, 5803, 6059, 1410, 6108, 1620, 4390, 234, 2982, 5982, 2429, 5148, 7162, 5755, 3136, 175, 6172, 4643, 5773, 80, 2985, 3574, 5553, 3364, 1503, 6244, 2074, 1541, 6586, 2010, 2751, 164, 6224, 2025, 6096, 1257, 4366, 5356, 4996, 7121, 1277, 3651, 3076, 3783, 6974, 692, 733, 744, 629, 1044, 5874, 2333, 859, 1677, 2023, 2729, 5675, 137, 6329, 846, 2071, 4474, 2831, 2805, 880, 1125, 6408, 1922, 6048, 6405, 4636, 5663, 2176, 3118, 2882, 1791, 1183, 745, 2444, 7165, 6163, 1130, 6989, 4187, 105, 616, 3619, 4642, 3760, 6174, 5863, 1865]        [0.774431, 0.7657779, 0.7654418, 0.7586538, 0.75765145, 0.75622255, 0.75387245, 0.7537848, 0.752055, 0.7515459, 0.751132, 0.75104064, 0.75036454, 0.75020015, 0.7485354, 0.74835086, 0.74833584, 0.74815947, 0.7479827, 0.747982, 0.7470428, 0.7463633, 0.74589694, 0.745704, 0.745267, 0.7447803, 0.744725, 0.74398893, 0.7432944, 0.7432766, 0.7427939, 0.74252474, 0.742417, 0.74239707, 0.7420739, 0.7419039, 0.7415648, 0.74129796, 0.74038553, 0.73997355, 0.7397634, 0.73882425, 0.7388134, 0.73840004, 0.73831725, 0.738283, 0.737738, 0.7374506, 0.7374104, 0.7374099, 0.7373049, 0.73705024, 0.73661757, 0.73660856, 0.73660856, 0.73660856, 0.73644245, 0.736059, 0.73586774, 0.73576295, 0.7357268, 0.7356725, 0.7353916, 0.73535186, 0.7352672, 0.73520756, 0.73473483, 0.73451936, 0.73436654, 0.73417914, 0.73410475, 0.73389065, 0.73387575, 0.73384523, 0.7338085, 0.7337475, 0.7335499, 0.73337823, 0.733353, 0.73333454, 0.7333176, 0.7330762, 0.73299396, 0.73235583, 0.73223346, 0.73214984, 0.73211503, 0.73199534, 0.7319912, 0.73147464, 0.7314663, 0.7311471, 0.7310847, 0.73101175, 0.7308252, 0.7308156, 0.7305696, 0.73056245, 0.73031694, 0.7302091]\n",
       "1  15   434          15 ||| free but i want my ||| min of life back ||| -1 \\n                   [2127, 2170, 3125, 2113, 718, 2121, 6895, 6026, 2160, 6782, 243, 6664, 2126, 1111, 5224, 6805, 629, 6357, 2184, 1812, 2084, 5175, 7037, 1179, 2259, 3416, 3799, 1251, 5773, 6337, 3363, 1342, 5147, 6274, 6172, 4676, 6713, 1554, 2158, 6884, 473, 614, 708, 3676, 3362, 4160, 2167, 2924, 1427, 3539, 7021, 4255, 6251, 1193, 5111, 2177, 5244, 3181, 7123, 3454, 3931, 5234, 7141, 2020, 3555, 1277, 2216, 3248, 3221, 2699, 1762, 6598, 6519, 110, 6553, 7137, 253, 5470, 5478, 1354, 3122, 175, 3212, 3156, 6761, 4560, 6741, 4354, 3952, 3708, 898, 4574, 1191, 1207, 3990, 2236, 244, 773, 3562, 6423]  [0.80159783, 0.80159783, 0.75432986, 0.7523407, 0.7521267, 0.7503412, 0.7487483, 0.74851257, 0.7460893, 0.7457087, 0.74384177, 0.7437999, 0.7436033, 0.7432044, 0.74240184, 0.74160695, 0.74076843, 0.74056995, 0.7400458, 0.73904514, 0.7377364, 0.7349162, 0.7347597, 0.7345468, 0.73429215, 0.73221064, 0.7314495, 0.72963405, 0.7294752, 0.7294397, 0.72929406, 0.7286548, 0.72816515, 0.72712684, 0.72669935, 0.7263777, 0.7255297, 0.72502124, 0.7247044, 0.72465146, 0.72463375, 0.72456336, 0.7242066, 0.7236022, 0.72344446, 0.72297347, 0.7227949, 0.7224593, 0.72219646, 0.722188, 0.7218343, 0.7215353, 0.721472, 0.7212204, 0.7209994, 0.72019124, 0.71988475, 0.719862, 0.71956116, 0.7195289, 0.71947646, 0.7192878, 0.7187964, 0.7187561, 0.7184754, 0.7182376, 0.7180848, 0.7177154, 0.7175374, 0.7174197, 0.7173748, 0.7172474, 0.7171042, 0.7170011, 0.71683997, 0.7166271, 0.7165545, 0.7164749, 0.7164495, 0.71596324, 0.71592975, 0.7158358, 0.7158158, 0.7157743, 0.7155573, 0.71547824, 0.7154068, 0.7153419, 0.7153045, 0.7151293, 0.7151145, 0.71509135, 0.71494764, 0.714921, 0.7148969, 0.71487635, 0.71478707, 0.71476185, 0.71471786, 0.7146066]   "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ite_placebo_window5.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "effective ||| subtle , but it is ||| . it's a quirky , ||| 1 "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_imdb_window5.topwd_unk_sentences_list[1845]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>treat_obj</th>\n",
       "      <th>control_obj</th>\n",
       "      <th>similarity</th>\n",
       "      <th>control_list</th>\n",
       "      <th>control_list_sim</th>\n",
       "      <th>control_lb_list</th>\n",
       "      <th>causal_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>1685</td>\n",
       "      <td>51 ||| on an igloo , formula ||| sank from quirky to jerky ||| -1 \\n</td>\n",
       "      <td>eager ||| on energy , and too ||| to be quirky at moments ||| -1 \\n</td>\n",
       "      <td>0.771962</td>\n",
       "      <td>[eager ||| on energy , and too ||| to be quirky at moments ||| -1 \\n, too ||| low on energy , and ||| eager to be quirky at ||| -1 \\n, charming ||| davis is funny , ||| and quirky in her feature ||| 1 \\n, charming ||| a ||| , quirky and leisurely paced ||| 1 \\n, effective ||| subtle , but it is ||| . it's a quirky , ||| 1 \\n, add ||| tries to ||| some spice to its quirky ||| -1 \\n, built ||| a premise , a joke ||| entirely from musty memories of ||| -1 \\n, funny ||| davis is ||| , charming and quirky in ||| 1 \\n, tries ||| , the harder that liman ||| to squeeze his story , ||| 1 \\n, unique ||| how the film knows what's ||| and quirky about canadians . ||| 1 \\n]</td>\n",
       "      <td>[0.7719615, 0.76660997, 0.7384485, 0.7363356, 0.7339607, 0.7187836, 0.7158764, 0.7153128, 0.712566, 0.7085526]</td>\n",
       "      <td>[-1, -1, 1, 1, 1, -1, -1, 1, 1, 1]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>2796</td>\n",
       "      <td>51 ||| formula ||| is so trite that even ||| -1 \\n</td>\n",
       "      <td>51 ||| formula ||| promises a new kind of ||| -1 \\n</td>\n",
       "      <td>0.853867</td>\n",
       "      <td>[51 ||| formula ||| promises a new kind of ||| -1 \\n, 51 ||| formula ||| has dulled your senses faster ||| -1 \\n, manages |||  ||| to be original , even ||| 1 \\n, problem ||| the ||| is that for the most ||| -1 \\n, none |||  ||| of this is half as ||| -1 \\n, jonah |||  ||| is only so-so . . ||| -1 \\n, problem ||| the ||| is that the movie has ||| -1 \\n, beauty ||| the ||| of the piece is that ||| 1 \\n, problem ||| the ||| with this film is that ||| -1 \\n, problem ||| the ||| with this film is that ||| -1 \\n]</td>\n",
       "      <td>[0.85386735, 0.8460399, 0.79239595, 0.7908349, 0.78753716, 0.785044, 0.7844373, 0.7817038, 0.77946806, 0.77946806]</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 1, -1, -1]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  term  sentence_id                                                             treat_obj                                                          control_obj  similarity                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   control_list                                                                                                    control_list_sim                         control_lb_list  causal_flag\n",
       "0  51   1685         51 ||| on an igloo , formula ||| sank from quirky to jerky ||| -1 \\n  eager ||| on energy , and too ||| to be quirky at moments ||| -1 \\n  0.771962    [eager ||| on energy , and too ||| to be quirky at moments ||| -1 \\n, too ||| low on energy , and ||| eager to be quirky at ||| -1 \\n, charming ||| davis is funny , ||| and quirky in her feature ||| 1 \\n, charming ||| a ||| , quirky and leisurely paced ||| 1 \\n, effective ||| subtle , but it is ||| . it's a quirky , ||| 1 \\n, add ||| tries to ||| some spice to its quirky ||| -1 \\n, built ||| a premise , a joke ||| entirely from musty memories of ||| -1 \\n, funny ||| davis is ||| , charming and quirky in ||| 1 \\n, tries ||| , the harder that liman ||| to squeeze his story , ||| 1 \\n, unique ||| how the film knows what's ||| and quirky about canadians . ||| 1 \\n]  [0.7719615, 0.76660997, 0.7384485, 0.7363356, 0.7339607, 0.7187836, 0.7158764, 0.7153128, 0.712566, 0.7085526]      [-1, -1, 1, 1, 1, -1, -1, 1, 1, 1]      0          \n",
       "1  51   2796         51 ||| formula ||| is so trite that even ||| -1 \\n                    51 ||| formula ||| promises a new kind of ||| -1 \\n                  0.853867    [51 ||| formula ||| promises a new kind of ||| -1 \\n, 51 ||| formula ||| has dulled your senses faster ||| -1 \\n, manages |||  ||| to be original , even ||| 1 \\n, problem ||| the ||| is that for the most ||| -1 \\n, none |||  ||| of this is half as ||| -1 \\n, jonah |||  ||| is only so-so . . ||| -1 \\n, problem ||| the ||| is that the movie has ||| -1 \\n, beauty ||| the ||| of the piece is that ||| 1 \\n, problem ||| the ||| with this film is that ||| -1 \\n, problem ||| the ||| with this film is that ||| -1 \\n]                                                                                                                                                              [0.85386735, 0.8460399, 0.79239595, 0.7908349, 0.78753716, 0.785044, 0.7844373, 0.7817038, 0.77946806, 0.77946806]  [-1, -1, 1, -1, -1, -1, -1, 1, -1, -1]  0          "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ite_window5['byconcat'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# match with whole sentence\n",
    "ds_data = pickle.load(open(data_path+'imdb/imdb_embedding_1_01.pkl', 'rb'))\n",
    "print('treat')\n",
    "df_ite_treat_sentence = get_data_matches(ds_data, matchby='treat', emb_by='concat') \n",
    "print('placebo')\n",
    "df_ite_placebo_sentence = get_data_matches(ds_data, matchby='placebo', emb_by='concat')\n",
    "\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(df_imdb_vocab_byconcat_window5,open(data_path+'imdb_vocab_byconcat_window5.pkl', 'wb'))\n",
    "# df_imdb_treat_byconcat_window5 = pickle.load(open(data_path+'imdb_treat_byconcat_window5.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb_treat_window5 = {}\n",
    "df_imdb_treat_window5['bymean'] = df_imdb_treat_bymean_window5\n",
    "df_imdb_treat_window5['bycontext'] = df_imdb_treat_bycontext_window5\n",
    "df_imdb_treat_window5['byconcat'] = df_imdb_treat_byconcat_window5\n",
    "\n",
    "pickle.dump(df_imdb_treat_window5, open(data_path+'/imdb/imdb_treat_ite_window5.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb_ite = {}\n",
    "df_imdb_ite['treat'] = df_imdb_treat\n",
    "df_imdb_ite['placebo'] = df_imdb_placebo\n",
    "df_imdb_ite['vocab'] = df_imdb_vocab\n",
    "df_imdb_ite['control'] = df_imdb_control\n",
    "pickle.dump(df_imdb_ite, open(data_path+'imdb_ite_1_01.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the number of terms that have empty context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>treat_obj</th>\n",
       "      <th>control_obj</th>\n",
       "      <th>similarity</th>\n",
       "      <th>control_list</th>\n",
       "      <th>control_list_sim</th>\n",
       "      <th>control_lb_list</th>\n",
       "      <th>causal_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>1685</td>\n",
       "      <td>51 ||| on an igloo , formula ||| sank from quirky to jerky ||| -1 \\n</td>\n",
       "      <td>eager ||| on energy , and too ||| to be quirky at moments ||| -1 \\n</td>\n",
       "      <td>0.771962</td>\n",
       "      <td>[eager ||| on energy , and too ||| to be quirky at moments ||| -1 \\n, too ||| low on energy , and ||| eager to be quirky at ||| -1 \\n, charming ||| davis is funny , ||| and quirky in her feature ||| 1 \\n, charming ||| a ||| , quirky and leisurely paced ||| 1 \\n, effective ||| subtle , but it is ||| . it's a quirky , ||| 1 \\n, add ||| tries to ||| some spice to its quirky ||| -1 \\n, built ||| a premise , a joke ||| entirely from musty memories of ||| -1 \\n, funny ||| davis is ||| , charming and quirky in ||| 1 \\n, tries ||| , the harder that liman ||| to squeeze his story , ||| 1 \\n, unique ||| how the film knows what's ||| and quirky about canadians . ||| 1 \\n]</td>\n",
       "      <td>[0.7719615, 0.76660997, 0.7384485, 0.7363356, 0.7339607, 0.7187836, 0.7158764, 0.7153128, 0.712566, 0.7085526]</td>\n",
       "      <td>[-1, -1, 1, 1, 1, -1, -1, 1, 1, 1]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>2796</td>\n",
       "      <td>51 ||| formula ||| is so trite that even ||| -1 \\n</td>\n",
       "      <td>51 ||| formula ||| promises a new kind of ||| -1 \\n</td>\n",
       "      <td>0.853867</td>\n",
       "      <td>[51 ||| formula ||| promises a new kind of ||| -1 \\n, 51 ||| formula ||| has dulled your senses faster ||| -1 \\n, manages |||  ||| to be original , even ||| 1 \\n, problem ||| the ||| is that for the most ||| -1 \\n, none |||  ||| of this is half as ||| -1 \\n, jonah |||  ||| is only so-so . . ||| -1 \\n, problem ||| the ||| is that the movie has ||| -1 \\n, beauty ||| the ||| of the piece is that ||| 1 \\n, problem ||| the ||| with this film is that ||| -1 \\n, problem ||| the ||| with this film is that ||| -1 \\n]</td>\n",
       "      <td>[0.85386735, 0.8460399, 0.79239595, 0.7908349, 0.78753716, 0.785044, 0.7844373, 0.7817038, 0.77946806, 0.77946806]</td>\n",
       "      <td>[-1, -1, 1, -1, -1, -1, -1, 1, -1, -1]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  term  sentence_id                                                             treat_obj                                                          control_obj  similarity                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   control_list                                                                                                    control_list_sim                         control_lb_list  causal_flag\n",
       "0  51   1685         51 ||| on an igloo , formula ||| sank from quirky to jerky ||| -1 \\n  eager ||| on energy , and too ||| to be quirky at moments ||| -1 \\n  0.771962    [eager ||| on energy , and too ||| to be quirky at moments ||| -1 \\n, too ||| low on energy , and ||| eager to be quirky at ||| -1 \\n, charming ||| davis is funny , ||| and quirky in her feature ||| 1 \\n, charming ||| a ||| , quirky and leisurely paced ||| 1 \\n, effective ||| subtle , but it is ||| . it's a quirky , ||| 1 \\n, add ||| tries to ||| some spice to its quirky ||| -1 \\n, built ||| a premise , a joke ||| entirely from musty memories of ||| -1 \\n, funny ||| davis is ||| , charming and quirky in ||| 1 \\n, tries ||| , the harder that liman ||| to squeeze his story , ||| 1 \\n, unique ||| how the film knows what's ||| and quirky about canadians . ||| 1 \\n]  [0.7719615, 0.76660997, 0.7384485, 0.7363356, 0.7339607, 0.7187836, 0.7158764, 0.7153128, 0.712566, 0.7085526]      [-1, -1, 1, 1, 1, -1, -1, 1, 1, 1]      0          \n",
       "1  51   2796         51 ||| formula ||| is so trite that even ||| -1 \\n                    51 ||| formula ||| promises a new kind of ||| -1 \\n                  0.853867    [51 ||| formula ||| promises a new kind of ||| -1 \\n, 51 ||| formula ||| has dulled your senses faster ||| -1 \\n, manages |||  ||| to be original , even ||| 1 \\n, problem ||| the ||| is that for the most ||| -1 \\n, none |||  ||| of this is half as ||| -1 \\n, jonah |||  ||| is only so-so . . ||| -1 \\n, problem ||| the ||| is that the movie has ||| -1 \\n, beauty ||| the ||| of the piece is that ||| 1 \\n, problem ||| the ||| with this film is that ||| -1 \\n, problem ||| the ||| with this film is that ||| -1 \\n]                                                                                                                                                              [0.85386735, 0.8460399, 0.79239595, 0.7908349, 0.78753716, 0.785044, 0.7844373, 0.7817038, 0.77946806, 0.77946806]  [-1, -1, 1, -1, -1, -1, -1, 1, -1, -1]  0          "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imdb_treat_byconcat_window5.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8882"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_imdb.topwd_unk_sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12996"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_imdb.placebowd_unk_sentences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['on', 'an', 'igloo', 'formula'], 'on an igloo , formula')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\w+', ds_imdb.topwd_unk_sentences_list[0].left_context), ds_imdb.topwd_unk_sentences_list[0].left_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_imdb = pickle.load(open(data_path+moniker+'_embedding_1_01_window5.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_empty_context(sentObj_list):\n",
    "    \"\"\"\n",
    "    A list of sentenceEdit object\n",
    "    \"\"\"\n",
    "    empty_id = []\n",
    "    for si,sent in enumerate(sentObj_list):\n",
    "        left_wds = re.findall('\\w+', sent.left_context)\n",
    "        right_wds = re.findall('\\w+', sent.right_context)\n",
    "        if(len(left_wds) == 0 or len(right_wds) == 0):\n",
    "            empty_id.append(si)\n",
    "    \n",
    "    print(\"%d (%.2f) items with empty contexts.\" % (len(empty_id), (len(empty_id)/len(sentObj_list))))\n",
    "    return empty_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1297 (0.15) items with empty contexts.\n",
      "1596 (0.12) items with empty contexts.\n"
     ]
    }
   ],
   "source": [
    "topwd_empty_ids = check_empty_context(ds_imdb.topwd_unk_sentences_list)\n",
    "placebowd_empty_ids = check_empty_context(ds_imdb.placebowd_unk_sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wds = []\n",
    "for eid in placebowd_empty_ids:\n",
    "    wds.append(ds_imdb.placebowd_unk_sentences_list[eid].remove_wd)\n",
    "\n",
    "len(Counter(wds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double check current embedding and ite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_imdb_window5 = pickle.load(open(data_path+'imdb/imdb_embedding_1_01_window5.pkl', 'rb'))\n",
    "ds_imdb = pickle.load(open(data_path+'imdb/imdb_embedding_1_01.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>i_th</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>simplistic , silly and tedious .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>it's so laddish and juvenile , only teenage boys could possibly find it funny .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification .</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                                                                                                                       text  i_th\n",
       "0 -1      simplistic , silly and tedious .                                                                                                           0   \n",
       "1 -1      it's so laddish and juvenile , only teenage boys could possibly find it funny .                                                            1   \n",
       "2 -1      exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable .  2   \n",
       "3 -1      [garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation .         3   \n",
       "4 -1      a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification .                                       4   "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_imdb.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8882, 12996, 156632)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_imdb_window5.topwd_unk_sentences_list), len(ds_imdb_window5.placebowd_unk_sentences_list), len(ds_imdb_window5.vocab_unk_sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20th ||| the relevance of these two ||| footnotes . ||| -1 ,\n",
       " 20th ||| key turning point of the ||| century , and returns again ||| 1 ,\n",
       " 20th ||| addressing the turn of the ||| century into the 21st . ||| 1 ]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_imdb_window5.placebowd_unk_sentences_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8882, 12996, 156632)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_imdb.topwd_unk_sentences_list), len(ds_imdb.placebowd_unk_sentences_list), len(ds_imdb.vocab_unk_sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20th ||| weiss and speck never make a convincing case for the relevance of these two ||| footnotes . ||| -1 ,\n",
       " 20th ||| while it regards 1967 as the key turning point of the ||| century , and returns again and again to images of dissidents in the streets , it's alarmingly current . ||| 1 ,\n",
       " 20th ||| in capturing the understated comedic agony of an ever-ruminating , genteel yet decadent aristocracy that can no longer pay its bills , the film could just as well be addressing the turn of the ||| century into the 21st . ||| 1 ]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_imdb.placebowd_unk_sentences_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ite_window5 = pickle.load(open(data_path+'imdb/imdb_treat_ite_window5.pkl', 'rb'))\n",
    "df_ite = pickle.load(open(data_path+'imdb/imdb_treat_ite_BERT.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['bymean', 'bycontext', 'byconcat']),\n",
       " dict_keys(['bymean', 'bycontext', 'byconcat']))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ite_window5.keys(), df_ite.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>treat_obj</th>\n",
       "      <th>control_obj</th>\n",
       "      <th>similarity</th>\n",
       "      <th>control_list</th>\n",
       "      <th>control_list_sim</th>\n",
       "      <th>control_lb_list</th>\n",
       "      <th>causal_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>1685</td>\n",
       "      <td>51 ||| on an igloo , formula ||| sank from quirky to jerky ||| -1 \\n</td>\n",
       "      <td>eager ||| on energy , and too ||| to be quirky at moments ||| -1 \\n</td>\n",
       "      <td>0.771962</td>\n",
       "      <td>[eager ||| on energy , and too ||| to be quirky at moments ||| -1 \\n, too ||| low on energy , and ||| eager to be quirky at ||| -1 \\n, charming ||| davis is funny , ||| and quirky in her feature ||| 1 \\n, charming ||| a ||| , quirky and leisurely paced ||| 1 \\n, effective ||| subtle , but it is ||| . it's a quirky , ||| 1 \\n, add ||| tries to ||| some spice to its quirky ||| -1 \\n, built ||| a premise , a joke ||| entirely from musty memories of ||| -1 \\n, funny ||| davis is ||| , charming and quirky in ||| 1 \\n, tries ||| , the harder that liman ||| to squeeze his story , ||| 1 \\n, unique ||| how the film knows what's ||| and quirky about canadians . ||| 1 \\n]</td>\n",
       "      <td>[0.7719615, 0.76660997, 0.7384485, 0.7363356, 0.7339607, 0.7187836, 0.7158764, 0.7153128, 0.712566, 0.7085526]</td>\n",
       "      <td>[-1, -1, 1, 1, 1, -1, -1, 1, 1, 1]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  term  sentence_id                                                             treat_obj                                                          control_obj  similarity                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   control_list                                                                                                control_list_sim                     control_lb_list  causal_flag\n",
       "0  51   1685         51 ||| on an igloo , formula ||| sank from quirky to jerky ||| -1 \\n  eager ||| on energy , and too ||| to be quirky at moments ||| -1 \\n  0.771962    [eager ||| on energy , and too ||| to be quirky at moments ||| -1 \\n, too ||| low on energy , and ||| eager to be quirky at ||| -1 \\n, charming ||| davis is funny , ||| and quirky in her feature ||| 1 \\n, charming ||| a ||| , quirky and leisurely paced ||| 1 \\n, effective ||| subtle , but it is ||| . it's a quirky , ||| 1 \\n, add ||| tries to ||| some spice to its quirky ||| -1 \\n, built ||| a premise , a joke ||| entirely from musty memories of ||| -1 \\n, funny ||| davis is ||| , charming and quirky in ||| 1 \\n, tries ||| , the harder that liman ||| to squeeze his story , ||| 1 \\n, unique ||| how the film knows what's ||| and quirky about canadians . ||| 1 \\n]  [0.7719615, 0.76660997, 0.7384485, 0.7363356, 0.7339607, 0.7187836, 0.7158764, 0.7153128, 0.712566, 0.7085526]  [-1, -1, 1, 1, 1, -1, -1, 1, 1, 1]  0          "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ite_window5['byconcat'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>treat_obj</th>\n",
       "      <th>control_obj</th>\n",
       "      <th>similarity</th>\n",
       "      <th>control_list</th>\n",
       "      <th>control_list_sim</th>\n",
       "      <th>control_lb_list</th>\n",
       "      <th>causal_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>1685</td>\n",
       "      <td>51 ||| in exactly 89 minutes , most of which passed as slowly as if i'd been sitting naked on an igloo , formula ||| sank from quirky to jerky to utter turkey . ||| -1 \\n</td>\n",
       "      <td>unique ||| the plot of the comeback curlers isn't very interesting actually , but what i like about men with brooms and what is kind of special is how the film knows what's ||| and quirky about canadians . ||| 1 \\n</td>\n",
       "      <td>0.778109</td>\n",
       "      <td>[unique ||| the plot of the comeback curlers isn't very interesting actually , but what i like about men with brooms and what is kind of special is how the film knows what's ||| and quirky about canadians . ||| 1 \\n, still ||| while easier to sit through than most of jaglom's self-conscious and gratingly irritating films , it's ||| tainted by cliches , painful improbability and murky points . ||| -1 \\n, eager ||| often likable , but just as often it's meandering , low on energy , and too ||| to be quirky at moments when a little old-fashioned storytelling would come in handy . ||| -1 \\n, too ||| often likable , but just as often it's meandering , low on energy , and ||| eager to be quirky at moments when a little old-fashioned storytelling would come in handy . ||| -1 \\n, flat ||| it's a lot to ask people to sit still for two hours and change watching such a character , especially when rendered in as ||| and impassive a manner as phoenix's . ||| -1 \\n, breathtaking ||| jackson tries to keep the plates spinning as best he can , but all the bouncing back and forth can't help but become a bit tedious -- even with the ||| landscapes and villainous varmints there to distract you from the ricocheting . ||| 1 \\n, treat ||| yes , 4ever is harmless in the extreme and it'll mute your kids for nearly 80 minutes , but why not just ||| the little yard apes to the real deal and take them to spirited away ? ||| -1 \\n, smarter ||| not only is undercover brother as funny , if not more so , than both austin powers films , but it's also one of the ||| , savvier spoofs to come along in some time . ||| 1 \\n, devoid ||| it's drained of life in an attempt to be sober and educational , and yet it's so ||| of realism that its lack of whistles and bells just makes it obnoxious and stiff . ||| -1 \\n, performances ||| is there a group of more self-absorbed women than the mother and daughters featured in this film ? i don't think so . nothing wrong with ||| here , but the whiney characters bugged me . ||| -1 \\n]</td>\n",
       "      <td>[0.77810943, 0.77367425, 0.7700913, 0.7654258, 0.76197994, 0.76025236, 0.7598947, 0.7573191, 0.7569665, 0.7566486]</td>\n",
       "      <td>[1, -1, -1, -1, -1, 1, -1, 1, -1, -1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  term  sentence_id                                                                                                                                                                   treat_obj                                                                                                                                                                                                             control_obj  similarity                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        control_list                                                                                                    control_list_sim                        control_lb_list  causal_flag\n",
       "0  51   1685         51 ||| in exactly 89 minutes , most of which passed as slowly as if i'd been sitting naked on an igloo , formula ||| sank from quirky to jerky to utter turkey . ||| -1 \\n  unique ||| the plot of the comeback curlers isn't very interesting actually , but what i like about men with brooms and what is kind of special is how the film knows what's ||| and quirky about canadians . ||| 1 \\n  0.778109    [unique ||| the plot of the comeback curlers isn't very interesting actually , but what i like about men with brooms and what is kind of special is how the film knows what's ||| and quirky about canadians . ||| 1 \\n, still ||| while easier to sit through than most of jaglom's self-conscious and gratingly irritating films , it's ||| tainted by cliches , painful improbability and murky points . ||| -1 \\n, eager ||| often likable , but just as often it's meandering , low on energy , and too ||| to be quirky at moments when a little old-fashioned storytelling would come in handy . ||| -1 \\n, too ||| often likable , but just as often it's meandering , low on energy , and ||| eager to be quirky at moments when a little old-fashioned storytelling would come in handy . ||| -1 \\n, flat ||| it's a lot to ask people to sit still for two hours and change watching such a character , especially when rendered in as ||| and impassive a manner as phoenix's . ||| -1 \\n, breathtaking ||| jackson tries to keep the plates spinning as best he can , but all the bouncing back and forth can't help but become a bit tedious -- even with the ||| landscapes and villainous varmints there to distract you from the ricocheting . ||| 1 \\n, treat ||| yes , 4ever is harmless in the extreme and it'll mute your kids for nearly 80 minutes , but why not just ||| the little yard apes to the real deal and take them to spirited away ? ||| -1 \\n, smarter ||| not only is undercover brother as funny , if not more so , than both austin powers films , but it's also one of the ||| , savvier spoofs to come along in some time . ||| 1 \\n, devoid ||| it's drained of life in an attempt to be sober and educational , and yet it's so ||| of realism that its lack of whistles and bells just makes it obnoxious and stiff . ||| -1 \\n, performances ||| is there a group of more self-absorbed women than the mother and daughters featured in this film ? i don't think so . nothing wrong with ||| here , but the whiney characters bugged me . ||| -1 \\n]  [0.77810943, 0.77367425, 0.7700913, 0.7654258, 0.76197994, 0.76025236, 0.7598947, 0.7573191, 0.7569665, 0.7566486]  [1, -1, -1, -1, -1, 1, -1, 1, -1, -1]  1          "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ite['byconcat'].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding with Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "def embed_all_sentences_USE(sentences):\n",
    "    USE_model = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")\n",
    "    \n",
    "    left_contexts = [s.left_context for s in sentences]\n",
    "    right_contexts = [s.right_context for s in sentences]\n",
    "    contexts = [s.left_context + ' ' + s.right_context for s in sentences]\n",
    "    words = [s.remove_wd for s in sentences]\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        left_embeddings = sess.run(USE_model(left_contexts))\n",
    "        right_embeddings = sess.run(USE_model(right_contexts))\n",
    "        context_embeddings = sess.run(USE_model(contexts))\n",
    "        word_embeddings = sess.run(USE_model(words))\n",
    "        \n",
    "    for i,s in enumerate(sentences):\n",
    "        s.left_embedding = left_embeddings[i]\n",
    "        s.right_embedding = right_embeddings[i]\n",
    "        s.context_embedding = context_embeddings[i]\n",
    "        s.word_embedding = word_embeddings[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_context = ds_imdb.topwd_unk_sentences_list[0].left_context + ' ' + ds_imdb.topwd_unk_sentences_list[0].right_context\n",
    "\n",
    "USE_model = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    test_embedding = sess.run(USE_model([test_context]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tf_bert",
   "language": "python",
   "name": "tf_bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
